{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define new diffusion proximal nested sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import ProxNest.logs as lg\n",
        "from . import resampling\n",
        "\n",
        "\n",
        "def DiffusionProxNestedSampling(X0, LikeliL, proxH, proxB, params, options):\n",
        "    r\"\"\"Executes the proximal nested sampling algorithm\n",
        "\n",
        "    Args:\n",
        "        X0 (np.ndarray): initialisation of the sample chain.\n",
        "\n",
        "        LikeliL (lambda): function to compute the likelihood value of a sample.\n",
        "\n",
        "        proxH (lambda): proximity operator of the prior.\n",
        "\n",
        "        proxB (lambda): proximity operator of the constraint :math:`\\ell_2`-ball.\n",
        "\n",
        "        params (dict): parameters for prior resampling subject to likelihood isocontour.\n",
        "\n",
        "        options (dict): parameters about number of samples, thinning factor, burnning numbers.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Evidence, sample trace).\n",
        "\n",
        "    Notes:\n",
        "        MATLAB version: Xiaohao Cai (21/02/2019)\n",
        "\n",
        "        Python version: Matthew Price (9/05/2022)\n",
        "    \"\"\"\n",
        "    sigma = options[\"sigma\"]\n",
        "    Phi = params[\"Phi\"]\n",
        "    y = params[\"y\"]\n",
        "\n",
        "    lg.info_log(\"Constructing lambda functions for resampling projections...\")\n",
        "\n",
        "    # Simulation setup\n",
        "    # Use backward-forward splitting to approximate proxPi using proxH and gradF\n",
        "    driftIniN = lambda X, delta, gamma: np.real(\n",
        "        (1 - delta / (2 * gamma)) * X \n",
        "        + delta / (2 * gamma) * proxH(X, gamma)\n",
        "    )\n",
        "    drift = lambda X, delta, lamb, tau, gamma, sigma: np.real(\n",
        "        (1 - delta / (2 * lamb) - delta / (2 * gamma)) * X\n",
        "        + delta / (2 * gamma) * proxH(X, gamma)\n",
        "        + delta / (2 * lamb) * proxB(X, np.sqrt(tau * 2 * sigma**2))\n",
        "    )\n",
        "\n",
        "    # Initialize variables\n",
        "    delta = options[\n",
        "        \"delta\"\n",
        "    ]  # delta controls the proposal variance, the step-length and Moreau approximation\n",
        "    if options['lamb'] is None:\n",
        "        lamb = 5 * delta  # lamb \\in [4*delta, 10*delta]\n",
        "    else:\n",
        "        lamb = options['lamb']\n",
        "    # If gamma not provided, copy the lamb value\n",
        "    if options[\"gamma\"] is None:\n",
        "        gamma = lamb\n",
        "    else:\n",
        "        gamma = options[\"gamma\"]\n",
        "    Xcur = X0  # set initial state as current state\n",
        "    tau_0 = -LikeliL(Xcur) * 1e-1\n",
        "\n",
        "    lg.info_log(\"Allocating memory and populating initial live-samples...\")\n",
        "\n",
        "    # Initialise arrays to store samples\n",
        "    # Indexing: sample, likelihood, weights\n",
        "    NumLiveSetSamples = options[\"samplesL\"]\n",
        "    NumDiscardSamples = options[\"samplesD\"]\n",
        "\n",
        "    Xtrace = {}\n",
        "\n",
        "    Xtrace[\"LiveSet\"] = np.zeros((NumLiveSetSamples, Xcur.shape[0], Xcur.shape[1]))\n",
        "    Xtrace[\"LiveSetL\"] = np.zeros(NumLiveSetSamples)\n",
        "\n",
        "    Xtrace[\"Discard\"] = np.zeros((NumDiscardSamples, Xcur.shape[0], Xcur.shape[1]))\n",
        "    Xtrace[\"DiscardL\"] = np.zeros(NumDiscardSamples)\n",
        "    Xtrace[\"DiscardW\"] = np.zeros(NumDiscardSamples)\n",
        "    Xtrace[\"DiscardPostProb\"] = np.zeros(NumDiscardSamples)\n",
        "\n",
        "    # Generate initialisation\n",
        "    j = 0\n",
        "    for ii in tqdm(range(200), desc=\"ProxNest || Initialise\"):\n",
        "        # P-ULA -- MARKOV CHAIN generating initialisation\n",
        "        Xcur = drift(\n",
        "            Xcur, delta, lamb, tau_0, gamma, sigma\n",
        "        ) + np.sqrt(delta) * np.random.randn(\n",
        "            Xcur.shape[0], Xcur.shape[1]\n",
        "        )\n",
        "\n",
        "    # Obtain samples from priors\n",
        "    for ii in tqdm(\n",
        "        range(2, NumLiveSetSamples * options[\"thinning\"] + options[\"burn\"]),\n",
        "        desc=\"ProxNest || Populate\",\n",
        "    ):\n",
        "\n",
        "        # P-ULA -- MARKOV CHAIN generating live samples\n",
        "        Xcur = driftIniN(Xcur, delta, gamma) + np.sqrt(delta) * np.random.randn(\n",
        "            Xcur.shape[0], Xcur.shape[1]\n",
        "        )\n",
        "\n",
        "        # Save sample (with thinning)\n",
        "        if (ii > options[\"burn\"]) and not (\n",
        "            (ii - options[\"burn\"]) % options[\"thinning\"]\n",
        "        ):\n",
        "            # Record the current sample in the live set and its likelihood\n",
        "            Xtrace[\"LiveSet\"][j] = Xcur\n",
        "            Xtrace[\"LiveSetL\"][j] = LikeliL(Xcur)\n",
        "\n",
        "            j += 1\n",
        "\n",
        "    lg.info_log(\"Executing primary nested resampling iterations...\")\n",
        "\n",
        "    # Reorder samples TODO: Make this more efficient!\n",
        "    Xtrace[\"LiveSet\"], Xtrace[\"LiveSetL\"] = resampling.reorder_samples(\n",
        "        Xtrace[\"LiveSet\"], Xtrace[\"LiveSetL\"]\n",
        "    )\n",
        "\n",
        "    # Update samples using the proximal nested sampling technique\n",
        "    for k in tqdm(range(NumDiscardSamples), desc=\"ProxNest || Sample\"):\n",
        "        # Compute the smallest threshold wrt live samples' likelihood\n",
        "        tau = -Xtrace[\"LiveSetL\"][-1]  # - 1e-2\n",
        "\n",
        "        # Randomly select a sample in the live set as a starting point\n",
        "        indNewSample = (\n",
        "            np.floor(np.random.rand() * (NumLiveSetSamples - 1)).astype(int) - 1\n",
        "        )\n",
        "        Xcur = Xtrace[\"LiveSet\"][indNewSample]\n",
        "\n",
        "        # Generate a new sample with likelihood larger than given threshould\n",
        "        Xcur = drift(\n",
        "            Xcur, delta, lamb, tau, gamma, sigma\n",
        "        ) + np.sqrt(delta) * np.random.randn(\n",
        "            Xcur.shape[0], Xcur.shape[1]\n",
        "        )\n",
        "\n",
        "        # check if the new sample is inside l2-ball (metropolis-hasting); if\n",
        "        # not, force the new sample into L2-ball\n",
        "        if np.sum(np.sum(np.abs(y - Phi.dir_op(Xcur)) ** 2)) > tau * 2 * sigma**2:\n",
        "            Xcur = proxB(Xcur, np.sqrt(tau * 2 * sigma**2))\n",
        "\n",
        "        # Record the sample discarded and its likelihood\n",
        "        Xtrace[\"Discard\"][k] = Xtrace[\"LiveSet\"][-1]\n",
        "        Xtrace[\"DiscardL\"][k] = Xtrace[\"LiveSetL\"][-1]\n",
        "\n",
        "        # Add the new sample to the live set and its likelihood\n",
        "        Xtrace[\"LiveSet\"][-1] = Xcur\n",
        "        Xtrace[\"LiveSetL\"][-1] = LikeliL(Xcur)\n",
        "\n",
        "        # Reorder the live samples TODO: Make this more efficient!\n",
        "        Xtrace[\"LiveSet\"], Xtrace[\"LiveSetL\"] = resampling.reorder_samples(\n",
        "            Xtrace[\"LiveSet\"], Xtrace[\"LiveSetL\"]\n",
        "        )\n",
        "\n",
        "    lg.info_log(\n",
        "        \"Estimating Bayesian evidence (with variance), posterior probabilies, and posterior mean...\"\n",
        "    )\n",
        "\n",
        "    # Bayesian evidence calculation\n",
        "    BayEvi = np.zeros(2)\n",
        "    Xtrace[\"DiscardW\"][0] = 1 / NumLiveSetSamples\n",
        "\n",
        "    # Compute the sample weight\n",
        "    for k in tqdm(range(NumDiscardSamples), desc=\"ProxNest || Compute Weights\"):\n",
        "        Xtrace[\"DiscardW\"][k] = np.exp(-(k + 1) / NumLiveSetSamples)\n",
        "\n",
        "    # Compute the volumn length for each sample using trapezium rule\n",
        "    discardLen = np.zeros(NumDiscardSamples)\n",
        "    discardLen[0] = (1 - np.exp(-2 / NumLiveSetSamples)) / 2\n",
        "\n",
        "    for i in tqdm(\n",
        "        range(1, NumDiscardSamples - 1), desc=\"ProxNest || Trapezium Integrate\"\n",
        "    ):\n",
        "        discardLen[i] = (Xtrace[\"DiscardW\"][i - 1] - Xtrace[\"DiscardW\"][i + 1]) / 2\n",
        "\n",
        "    discardLen[-1] = (\n",
        "        np.exp(-(NumDiscardSamples - 1) / NumLiveSetSamples)\n",
        "        - np.exp(-(NumDiscardSamples + 1) / NumLiveSetSamples)\n",
        "    ) / 2\n",
        "    # volume length of the last discarded sample\n",
        "\n",
        "    liveSampleLen = np.exp(-(NumDiscardSamples) / NumLiveSetSamples)\n",
        "    # volume length of the living sample\n",
        "\n",
        "    # Apply the disgarded sample for Bayesian evidence value computation\n",
        "    vecDiscardLLen = Xtrace[\"DiscardL\"] + np.log(discardLen)\n",
        "\n",
        "    # Apply the final live set samples for Bayesian evidence value computation\n",
        "    vecLiveSetLLen = Xtrace[\"LiveSetL\"] + np.log(liveSampleLen / NumLiveSetSamples)\n",
        "\n",
        "    # #   ------- Way 1: using discarded and living samples --------\n",
        "    # # Get the maximum value of the exponents for all the samples\n",
        "    # maxAllSampleLLen = max(max(vecDiscardLLen),max(vecLiveSetLLen))\n",
        "\n",
        "    # # Compute the Bayesian evidence value using discarded and living samples\n",
        "    # BayEvi[0] = maxAllSampleLLen + np.log(np.sum(np.exp(vecDiscardLLen-maxAllSampleLLen)) + np.sum(np.exp(vecLiveSetLLen-maxAllSampleLLen)))\n",
        "\n",
        "    # ------- Way 2: using discarded samples --------\n",
        "    # Get the maximum value of the exponents for the discarded samples\n",
        "    maxDiscardLLen = np.max(vecDiscardLLen)\n",
        "\n",
        "    # Compute the Bayesian evidence value using discarded and living samples\n",
        "    BayEvi[0] = maxDiscardLLen + np.log(np.sum(np.exp(vecDiscardLLen - maxDiscardLLen)))\n",
        "\n",
        "    # Extimate the error of the computed Bayesian evidence\n",
        "    entropyH = 0\n",
        "\n",
        "    for k in tqdm(range(NumDiscardSamples), desc=\"ProxNest || Estimate Variance\"):\n",
        "        temp1 = np.exp(Xtrace[\"DiscardL\"][k] + np.log(discardLen[k]) - BayEvi[0])\n",
        "        entropyH = entropyH + temp1 * (Xtrace[\"DiscardL\"][k] - BayEvi[0])\n",
        "\n",
        "    # Evaluate the evidence variance\n",
        "    BayEvi[1] = np.sqrt(np.abs(entropyH) / NumLiveSetSamples)\n",
        "\n",
        "    # Compute the posterior probability for each discarded sample\n",
        "    for k in tqdm(range(NumDiscardSamples), desc=\"ProxNest || Compute Posterior Mean\"):\n",
        "        Xtrace[\"DiscardPostProb\"][k] = np.exp(\n",
        "            Xtrace[\"DiscardL\"][k] + np.log(discardLen[k]) - BayEvi[0]\n",
        "        )\n",
        "\n",
        "    # Compute the posterior mean of the discarded samples -- optimal solution\n",
        "    Xtrace[\"DiscardPostMean\"] = np.zeros((Xcur.shape[0], Xcur.shape[1]))\n",
        "    for k in range(NumDiscardSamples):\n",
        "        Xtrace[\"DiscardPostMean\"] += Xtrace[\"DiscardPostProb\"][k] * Xtrace[\"Discard\"][k]\n",
        "\n",
        "    return BayEvi, Xtrace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "convex_uq",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "2bb75ebd6ceb1eff2ce987e124c91bc6f99e62fd1930d98a82dc138614104eef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
